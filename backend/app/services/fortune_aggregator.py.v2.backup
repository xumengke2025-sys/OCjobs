"""
å‘½è¿æ€»ç»“å®˜æœåŠ¡
è´Ÿè´£èšåˆ 49 ä½å¤§å¸ˆçš„æ¨æ¼”ç»“æœï¼Œæå–å…±è¯†ã€å†²çªåŠå›¾è°±æ•°æ®
"""

import json
import concurrent.futures
import random
import re
import datetime
from typing import Dict, Any, List, Optional
from ..utils.llm_client import LLMClient
from ..utils.logger import get_logger

logger = get_logger('wannian.fortune_aggregator')

GRAPH_PROMPT = """
# Role: å‘½è¿æ¶æ„å¸ˆ
# Task: åŸºäº49ä½å¤§å¸ˆé¢„æµ‹ï¼Œæ„å»ºæœªæ¥ {{future_years}} å¹´çš„"èµ›åšå¤©æœºå›¾è°±"JSONã€‚

# CRITICAL RULES - å¿…é¡»ä¸¥æ ¼éµå®ˆï¼š
1. **ä¸¥ç¦ç¼–é€ **ï¼šæ‰€æœ‰å†…å®¹å¿…é¡»åŸºäºæä¾›çš„49ä½å¤§å¸ˆæ¨æ¼”æ–‡æœ¬ï¼Œç¦æ­¢è™šæ„ä»»ä½•é¢„æµ‹å†…å®¹ã€‚
2. **æ ‡é¢˜å¿…é¡»æ¦‚æ‹¬å†…å®¹**ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„ `name` å¿…é¡»å‡†ç¡®æ¦‚æ‹¬ `description` çš„æ ¸å¿ƒä¸»é¢˜ï¼Œç”¨æˆ·çœ‹åˆ°æ ‡é¢˜å°±èƒ½ç†è§£å†…å®¹ä¸»æ—¨ã€‚

# Requirements:
1. **ç»“æ„åŒ–åˆ†ç±» (Crucial)**ï¼šæ¯å¹´æ¯ä¸ªç»´åº¦ï¼ˆäº‹ä¸š/è´¢å¯Œ/æƒ…æ„Ÿ/å¥åº·ï¼‰å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ç§ç±»å‹çš„èŠ‚ç‚¹ï¼š
   - **consensus (å…±è¯†)**ï¼šè‡³å°‘ 60% å¤§å¸ˆè¾¾æˆçš„æ ¸å¿ƒå…±è¯†ã€‚`master_name` å›ºå®šä¸º "ä¼—å¸ˆå…±è¯†"ã€‚æ¯ä¸ªç»´åº¦æ¯å¹´ 1 ä¸ªã€‚
   - **unique (ç‹¬ç‰¹è§‚ç‚¹)**ï¼šæŸä½å¤§å¸ˆæå‡ºçš„ä¸ä¼—ä¸åŒçš„æ·±åˆ»æ´å¯Ÿã€‚`master_name` å¿…é¡»æ˜¯å…·ä½“å¤§å¸ˆåã€‚**æ¯ä¸ªç»´åº¦æ¯å¹´ 1-3 ä¸ª**ã€‚
   - **variable (å‘½ç†å˜æ•°)**ï¼šé¢„æµ‹ä¸­çš„ä¸ç¡®å®šé¡¹ã€å†²çªç‚¹æˆ–è½¬æŠ˜å¥‘æœºã€‚`master_name` å¿…é¡»æ˜¯å…·ä½“å¤§å¸ˆåã€‚**æ¯ä¸ªç»´åº¦æ¯å¹´ 1-2 ä¸ª**ã€‚

2. **èŠ‚ç‚¹æ ‡é¢˜è¦æ±‚ (CRITICAL - æå…¶é‡è¦)**ï¼š
   - `name` å­—æ®µå¿…é¡»æ˜¯ **2-5 ä¸ªå­—çš„å‘½ç†ç‰¹å¾æ ‡é¢˜**ï¼Œç”¨äºåœ¨å›¾è°±ä¸­ç›´æ¥å±•ç¤º
   - **æ ‡é¢˜å¿…é¡»å‡†ç¡®æ¦‚æ‹¬ description çš„å†…å®¹**ï¼šçœ‹åˆ°æ ‡é¢˜å°±èƒ½çŸ¥é“æè¿°è®²çš„æ˜¯ä»€ä¹ˆ
   - å¿…é¡»æ˜¯å…·ä½“çš„å‘½ç†ç‰¹å¾è¯ï¼Œå¦‚ï¼š"æ™‹å‡æœºé‡"ã€"è´µäººç›¸åŠ©"ã€"æ¡ƒèŠ±æ—ºç››"ã€"è„¾èƒƒè°ƒå…»"ã€"åè´¢å¯æœŸ"ã€"å°äººé˜²èŒƒ"
   - ç»å¯¹ç¦æ­¢æŠ½è±¡è¡¨è¾¾ï¼š"äº‹ä¸šå…±è¯†"ã€"è´¢å¯Œå˜åŒ–"ã€"è¿åŠ¿èµ°å‘"ã€"å¥åº·çŠ¶å†µ"ã€"æƒ…æ„Ÿè¿åŠ¿"
   - **å¥½æ ‡é¢˜ç¤ºä¾‹**ï¼š
     - æè¿°è®²æ™‹å‡æœºä¼š â†’ æ ‡é¢˜ï¼š"æ™‹å‡æœºé‡"
     - æè¿°è®²é‡åˆ°è´µäººå¸®åŠ© â†’ æ ‡é¢˜ï¼š"è´µäººç›¸åŠ©"  
     - æè¿°è®²éœ€è¦é˜²èŒƒå°äºº â†’ æ ‡é¢˜ï¼š"å°äººé˜²èŒƒ"
     - æè¿°è®²æ¡ƒèŠ±è¿æ—º â†’ æ ‡é¢˜ï¼š"æ¡ƒèŠ±æ—ºç››"
     - æè¿°è®²è„¾èƒƒéœ€è¦è°ƒå…» â†’ æ ‡é¢˜ï¼š"è„¾èƒƒè°ƒå…»"
   - **åæ ‡é¢˜ç¤ºä¾‹ï¼ˆä¸¥ç¦ï¼‰**ï¼š
     - æè¿°è®²æ™‹å‡æœºä¼š â†’ æ ‡é¢˜ï¼š"äº‹ä¸šå‘å±•"ï¼ˆå¤ªç¬¼ç»Ÿï¼‰
     - æè¿°è®²å…·ä½“æŠ•èµ„ â†’ æ ‡é¢˜ï¼š"è´¢å¯Œå˜åŒ–"ï¼ˆä¸å…·ä½“ï¼‰

3. **èŠ‚ç‚¹æè¿°è¦æ±‚ (CRITICAL)**ï¼š
   - æ¯ä¸ªèŠ‚ç‚¹çš„ `description` å¿…é¡»åŒ…å« **200-350 å­—**çš„è¯¦ç»†åˆ†æï¼Œ**ä¸¥ç¦çŸ­äº150å­—**
   - **å¿…é¡»åŸºäºåŸå§‹æŠ¥å‘Š**ï¼šä»49ä½å¤§å¸ˆçš„æ¨æ¼”æ–‡æœ¬ä¸­æå–å…·ä½“å†…å®¹ï¼Œ**ä¸¥ç¦ç¼–é€ **
   - **å¯¹äº unique/variable èŠ‚ç‚¹ï¼ˆæå…¶é‡è¦ï¼‰**ï¼š
     * ç›´æ¥å¼•ç”¨è¯¥å¤§å¸ˆåŸæ–‡ä¸­çš„æ ¸å¿ƒæ®µè½ï¼ˆ100-200å­—ï¼‰
     * ä¿ç•™è¯¥å¤§å¸ˆçš„**åŸå§‹è¯­é£å’Œä¸“ä¸šæœ¯è¯­**ï¼ˆå¦‚"å®˜æ€æ··æ‚"ã€"å¤©å…‹åœ°å†²"ã€"ä¸‰æ–¹å››æ­£"ç­‰ï¼‰
     * **ç»å¯¹ç¦æ­¢**å°†å…¶æ”¹å†™ä¸ºé€šç”¨çš„"è¿åŠ¿å˜å¥½/å˜å"åºŸè¯
   - æè¿°ä¸­**ç¦æ­¢**å‡ºç°"è¿™ä½å¤§å¸ˆè®¤ä¸º"ã€"æ ¹æ®é¢„æµ‹"ç­‰åºŸè¯å¥—è¯ï¼Œç›´æ¥é™ˆè¿°è§‚ç‚¹
   - å¿…é¡»åŒ…å«ï¼šå…·ä½“æ—¶é—´èŠ‚ç‚¹ã€äº‹ä»¶æè¿°ã€åŸå› åˆ†æã€åº”å¯¹å»ºè®®

4. **æº¯æº**ï¼šé™¤å…±è¯†èŠ‚ç‚¹å¤–ï¼Œå¿…é¡»ç²¾å‡†æŒ‡æ˜è§‚ç‚¹å‡ºè‡ªå“ªä½å¤§å¸ˆã€‚

5. **å…³è”æ€§**ï¼šå¿…é¡»æ„å»ºèŠ‚ç‚¹é—´çš„ `edges`ã€‚å…³ç³»ç±»å‹åŒ…æ‹¬ï¼š
   - "å› æœ" (Causal): ä¸€ä¸ªäº‹ä»¶å¯¼è‡´å¦ä¸€ä¸ªã€‚
   - "å¯¹å†²" (Conflict): ä¸¤ä¸ªç»´åº¦é—´çš„çŸ›ç›¾æˆ–è§‚ç‚¹å†²çªã€‚
   - "äº’è¡¥" (Complement): äº’ç›¸ä¿ƒè¿›ã€‚
   - "æ—¶åº" (Sequence): è·¨å¹´ä»½çš„å½±å“ã€‚

6. **49ä½å¤§å¸ˆæ„è§èšåˆé€»è¾‘ (é‡è¦)**ï¼š
   - ä½ ä¼šæ”¶åˆ° 49 ä½å¤§å¸ˆçš„æ¨æ¼”æ‘˜è¦æ–‡æœ¬ï¼Œæ¯æ®µä»¥ `--- ã€å¤§å¸ˆåã€‘ ---` å¼€å¤´
   - **ä¸¥æ ¼åŸºäºåŸæ–‡**ï¼šæ‰€æœ‰èŠ‚ç‚¹å†…å®¹å¿…é¡»ä»åŸæ–‡ä¸­æå–ï¼Œç¦æ­¢ç¼–é€ 
   - è¯·æŒ‰ã€Œå¹´ä»½ Ã— ç»´åº¦ã€ï¼ˆå¦‚ 2026å¹´-äº‹ä¸šï¼‰å¯¹æ‰€æœ‰å†…å®¹è¿›è¡Œåˆ†ç»„ï¼š
     1) æ‰¾å‡ºæåŠè¯¥å¹´ä»½è¯¥ç»´åº¦çš„æ‰€æœ‰å¤§å¸ˆæ®µè½
     2) å¦‚æœå¤šä½å¤§å¸ˆæåˆ°ç›¸ä¼¼å†…å®¹ â†’ ç”Ÿæˆ consensus èŠ‚ç‚¹
     3) å¦‚æœæŸä½å¤§å¸ˆæœ‰ç‹¬ç‰¹è§è§£ â†’ ç”Ÿæˆ unique èŠ‚ç‚¹ï¼ˆä¿ç•™åŸæ–‡è¯­é£ï¼‰
     4) å¦‚æœå­˜åœ¨åˆ†æ­§æˆ–ä¸ç¡®å®šæ€§ â†’ ç”Ÿæˆ variable èŠ‚ç‚¹

# Output JSON Structure:
{
  "graph_data": {
    "nodes": [{"id": "n1", "properties": {"name": "2-5å­—å‘½ç†ç‰¹å¾æ ‡é¢˜", "time": "2026å¹´", "description": "200-350å­—è¯¦ç»†åˆ†æ...", "master_name": "ä¼—å¸ˆå…±è¯†|å…·ä½“å¤§å¸ˆå", "school_source": "..", "type": "consensus|unique|variable", "impact": 1-10, "dimension": "career|wealth|emotion|health"}}],
    "edges": [{"source": "n1", "target": "n2", "label": "å…³ç³»æè¿°", "type": "causal|conflict|complement|sequence"}]
  },
  "consensus": ["å…±è¯†ç‚¹1", "å…±è¯†ç‚¹2"], 
  "conflicts": ["å†²çªç‚¹1", "å†²çªç‚¹2"]
}
"""

SUMMARY_PROMPT = """
# Role: å‘½è¿æ€»ç»“å®˜
# Task: åŸºäº49ä½å¤§å¸ˆé¢„æµ‹ï¼Œæ’°å†™ä¸€ä»½å…¨æ¡ˆè‡´è¾ Markdownã€‚
# Requirements:
1. **ä¸€è‡´æ€§**ï¼šä¸¥ç¦ç¼–é€ ã€‚ç¡®ä¿æ¯ä¸ªè§‚ç‚¹ä¸äº‹å®é€»è¾‘å»åˆã€‚
2. **ç»“æ„**ï¼šæŒ‰å¹´ä»½åŠç»´åº¦(äº‹ä¸š/è´¢å¯Œ/æƒ…æ„Ÿ/å¥åº·)ç»„ç»‡ã€‚
3. **é£æ ¼**ï¼šä¼˜ç¾è‡ªç„¶ï¼Œå°†ç»“æ„åŒ–é€»è¾‘è½¬åŒ–ä¸ºæ„Ÿæ€§è§£è¯»ã€‚
# Output Format:
## ğŸ”® æ ¸å¿ƒå…±è¯†ä¸ç‹¬ç‰¹ä¿¡å·
### ğŸŒŒ æ ¸å¿ƒå…±è¯†
...
### âš¡ ç‹¬ç‰¹ä¿¡å·
...
## ğŸ“… æœªæ¥ {{future_years}} å¹´æ—¶ç©ºæ¨æ¼”è¡¨
### 2026å¹´ (ä¸™åˆ)
#### ğŸ’¼ äº‹ä¸š
- **ä¼—å¸ˆå…±è¯†**ï¼š...
- **ç‹¬ç‰¹è§†è§’**ï¼š...
...
"""

class FortuneAggregator:
    """å‘½è¿æ€»ç»“å®˜"""
    
    def __init__(self, llm_client: Optional[LLMClient] = None):
        self.llm = llm_client or LLMClient()
    
    def aggregate_reports(
        self, 
        user_data: Dict[str, Any], 
        reports: Dict[str, Dict[str, Any]],
        on_progress: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        å¹¶è¡ŒèšåˆæŠ¥å‘Šï¼šåŒæ—¶ç”Ÿæˆå›¾è°±å’Œæ–‡æœ¬ï¼Œå¤§å¹…æå‡é€Ÿåº¦
        """
        if on_progress:
            on_progress(92, "æ­£åœ¨æ‹¨åŠ¨æ˜Ÿç›˜ï¼Œèƒå– 49 ä½å¤§å¸ˆæ¨æ¼”ç²¾è¦...")
            
        future_years = user_data.get("future_years", 3)
        
        reports_text_preview = ""
        full_reports_text = ""
        for agent_id, data in reports.items():
            # å¢åŠ é¢„è§ˆé•¿åº¦ä»¥ä¿ç•™æ›´å¤šç‹¬ç‰¹è§‚ç‚¹ï¼Œé˜²æ­¢ LLM åªæœ‰å¼€å¤´å¥—è¯
            content_preview = data['content'][:800] + "..." if len(data['content']) > 800 else data['content']
            reports_text_preview += f"\n--- ã€{data['name']}ã€‘ ---\n{content_preview}\n"
            full_reports_text += f"\n--- ã€{data['name']}ã€‘ ---\n{data['content']}\n"
        
        user_context = f"ç”¨æˆ·ä¿¡æ¯: {json.dumps(user_data, ensure_ascii=False)}\næ¨æ¼”æ‘˜è¦: {reports_text_preview}"

        reports_list = list(reports.values())
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªè€—æ—¶çš„ LLM ä»»åŠ¡
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ with è¯­å¥ï¼Œä»¥ä¾¿åœ¨è¶…æ—¶å‘ç”Ÿæ—¶èƒ½é€šè¿‡ shutdown(wait=False) å¼ºåˆ¶ä¸ç­‰å¾…åƒµå°¸çº¿ç¨‹
        executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
        try:
            if on_progress:
                on_progress(94, "æ­£åœ¨æ ¡å‡†å¤©æ˜Ÿæ–¹ä½ï¼Œå‡èšæ—¶ç©ºå¤©æœºå›¾è°±...")
            
            # ä»»åŠ¡ 1: ç”Ÿæˆå›¾è°± JSON
            graph_future = executor.submit(
                self.llm.chat_json, 
                [{"role": "system", "content": GRAPH_PROMPT.replace("{{future_years}}", str(future_years))},
                 {"role": "user", "content": user_context}],
                temperature=0.3, use_boost=True
            )
            
            # ä»»åŠ¡ 2: ç”Ÿæˆæ€»ç»“æ–‡æœ¬
            summary_future = executor.submit(
                self.llm.chat,
                [{"role": "system", "content": SUMMARY_PROMPT.replace("{{future_years}}", str(future_years))},
                 {"role": "user", "content": f"è¯·åŸºäºå¤§å¸ˆæ¨æ¼”æ‘˜è¦æ’°å†™æŠ¥å‘Šï¼š\n{user_context}"}],
                temperature=0.7, use_boost=True
            )

            # è·å–å›¾è°±ç»“æœ - å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 180s
            try:
                graph_result = graph_future.result(timeout=180)
                logger.info("å›¾è°± JSON ç”ŸæˆæˆåŠŸ")
            except concurrent.futures.TimeoutError:
                logger.error("å›¾è°±ç”Ÿæˆä»»åŠ¡è¶…æ—¶ (180s)")
                raise Exception("å›¾è°±ç”Ÿæˆè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
            except Exception as e:
                logger.error(f"å›¾è°±ç”Ÿæˆå‘ç”Ÿå¼‚å¸¸: {str(e)}")
                raise

            if on_progress:
                on_progress(97, "å¤©æœºæ­£åœ¨å‡èšï¼Œæ­£åœ¨ç¼–æ’°å…¨æ¡ˆè‡´è¾...")

            # è·å–æ€»ç»“ç»“æœ - å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 180s
            try:
                summary_text = summary_future.result(timeout=180)
                logger.info("æ€»ç»“æ–‡æœ¬ç”ŸæˆæˆåŠŸ")
            except concurrent.futures.TimeoutError:
                logger.error("æ€»ç»“æ–‡æœ¬ç”Ÿæˆè¶…æ—¶ (180s)ï¼Œä½¿ç”¨é»˜è®¤å ä½æ–‡æœ¬")
                summary_text = "ï¼ˆå¤©æœºè¿è¡Œç¨æ˜¾è¿Ÿæ»ï¼Œç”±äºæ¨æ¼”è§„æ¨¡å·¨å¤§ï¼Œæ€»ç»“ç”Ÿæˆè¶…æ—¶ã€‚è¯·ç›´æ¥æŸ¥é˜…ä¸‹æ–¹è¯¦ç»†å›¾è°±ä¸å¤§å¸ˆæŠ¥å‘Šï¼‰"
            except Exception as e:
                logger.error(f"æ€»ç»“ç”Ÿæˆå‘ç”Ÿå¼‚å¸¸: {str(e)}")
                summary_text = "ï¼ˆå¤©æœºè¿è¡Œç¨æ˜¾è¿Ÿæ»ï¼Œè¯·ç›´æ¥æŸ¥é˜…ä¸‹æ–¹è¯¦ç»†å›¾è°±ä¸å¤§å¸ˆæŠ¥å‘Šï¼‰"
        
        finally:
            # å…³é”®ä¿®å¤ï¼šä¸å†ç­‰å¾…çº¿ç¨‹ç»“æŸï¼Œé˜²æ­¢å›  LLM å®¢æˆ·ç«¯æŒ‚æ­»å¯¼è‡´ä¸»çº¿ç¨‹æ°¸ä¹…é˜»å¡
            executor.shutdown(wait=False)

        # æ•°æ®æ¸…æ´—ä¸æ ¡éªŒ - ä¸å†ç”Ÿæˆæ–°èŠ‚ç‚¹ï¼Œåªåšæ ¡éªŒ
        if not isinstance(graph_result, dict) or not graph_result.get("graph_data", {}).get("nodes"):
            logger.error("å›¾è°±ç”Ÿæˆç»“æœå¼‚å¸¸æˆ–ä¸ºç©º")
            raise Exception("å›¾è°±ç”Ÿæˆå¤±è´¥ï¼Œæœªèƒ½è·å–æœ‰æ•ˆèŠ‚ç‚¹æ•°æ®")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ graph_result å†…å®¹
        logger.info(f"æ¸…æ´—å‰å›¾è°±èŠ‚ç‚¹æ•°: {len(graph_result.get('graph_data', {}).get('nodes', []))}")
        logger.info(f"æ¸…æ´—å‰å›¾è°±è¾¹æ•°: {len(graph_result.get('graph_data', {}).get('edges', []))}")
            
        # é¢„å¤„ç†æŠ¥å‘Šåˆ—è¡¨ï¼Œæå–æ®µè½
        preprocessed_reports = []
        for r in reports_list:
            content = r.get('content', '')
            paras = [p.strip() for p in re.split(r'[\nã€‚ï¼ï¼Ÿ]', content) if p.strip()]
            preprocessed_reports.append({
                "name": r.get('name', 'æœªçŸ¥å¤§å¸ˆ'),
                "paragraphs": paras
            })

        graph_result = self._sanitize_result(graph_result, future_years, preprocessed_reports)
        
        final_result = graph_result
        final_result["summary_text"] = summary_text
        
        # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤è¿”å›æ•°æ®ç»“æ„
        logger.info("="*60)
        logger.info("æœ€ç»ˆè¿”å›æ•°æ®ç»“æ„æ£€æŸ¥ï¼š")
        logger.info(f"final_result keys: {list(final_result.keys())}")
        logger.info(f"summary_text é•¿åº¦: {len(summary_text) if summary_text else 0}")
        logger.info(f"graph_data æ˜¯å¦å­˜åœ¨: {('graph_data' in final_result)}")
        if 'graph_data' in final_result:
            logger.info(f"graph_data keys: {list(final_result['graph_data'].keys())}")
            logger.info(f"nodes æ•°é‡: {len(final_result['graph_data'].get('nodes', []))}")
            logger.info(f"edges æ•°é‡: {len(final_result['graph_data'].get('edges', []))}")
            # è¾“å‡ºå‰3ä¸ªèŠ‚ç‚¹çš„æ‘˜è¦
            for i, node in enumerate(final_result['graph_data'].get('nodes', [])[:3]):
                logger.info(f"èŠ‚ç‚¹ {i+1}: id={node.get('id')}, name={node.get('properties', {}).get('name')}, type={node.get('properties', {}).get('type')}")
        logger.info("="*60)
        
        if on_progress:
            on_progress(100, "å¤©æœºå·²ç°ï¼Œå…¨æ¡ˆæ¨æ¼”ç¼–æ’°å®Œæˆ")
            
        return final_result

    def _extract_rich_description(self, preprocessed_reports: List[Dict[str, Any]], dimension: str, year: str, exclude_texts: List[str] = None) -> tuple:
        """ä»ä¸Šä¸‹æ–‡ä¸­æŠ“å–å†…å®¹ä¸°å¯Œçš„æè¿°æ–‡æœ¬åŠå¯¹åº”çš„å¤§å¸ˆå§“å
        
        Args:
            preprocessed_reports: é¢„å¤„ç†åçš„æŠ¥å‘Šåˆ—è¡¨ [{'name': '...', 'paragraphs': [...]}, ...]
            exclude_texts: å·²ä½¿ç”¨çš„æè¿°åˆ—è¡¨ï¼Œé¿å…é‡å¤æå–ç›¸åŒå†…å®¹
        """
        if exclude_texts is None:
            exclude_texts = []
            
        keywords = {
            "career": ["äº‹ä¸š", "å·¥ä½œ", "æ™‹å‡", "èŒåœº", "åˆ›ä¸š", "åå£°", "å®˜", "å­¦ä¸š", "èŒä½", "å‡è¿", "ä¸šç»©"],
            "wealth": ["è´¢å¯Œ", "é‡‘é’±", "æŠ•èµ„", "æ”¶ç›Š", "ç ´è´¢", "è´¢è¿", "é‡‘", "åˆ©", "ç†è´¢", "èµ„äº§", "æ”¶å…¥"],
            "emotion": ["æ„Ÿæƒ…", "å©šå§»", "æ‹çˆ±", "æ¡ƒèŠ±", "ä¼´ä¾£", "å®¶åº­", "æƒ…", "ç¼˜", "çˆ±æƒ…", "é…å¶", "å§»ç¼˜"],
            "health": ["å¥åº·", "èº«ä½“", "ç–¾ç—…", "å…»ç”Ÿ", "å¹³å®‰", "ç–¾", "å®‰", "ä½“è´¨", "è°ƒå…»", "åŒ»"]
        }
        
        target_keys = keywords.get(dimension, [])
        candidates = []
        
        for report in preprocessed_reports:
            master_name = report.get('name', 'æœªçŸ¥å¤§å¸ˆ')
            paragraphs = report.get('paragraphs', [])
            
            for para in paragraphs:
                if len(para) < 30: continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«ä½¿ç”¨
                if any(para[:50] in used for used in exclude_texts):
                    continue
                
                score = 0
                has_year = year[:4] in para if year else False
                key_count = sum(1 for k in target_keys if k in para)
                
                if has_year:
                    score += 50
                score += key_count * 10
                score += min(len(para), 200) // 10
                
                if score > 0:
                    candidates.append((score, para, master_name))
        
        if candidates:
            candidates.sort(key=lambda x: -x[0])
            return candidates[0][1][:250], candidates[0][2]
        
        return "", "å¤§å¸ˆå…±é¸£"
    
    def _extract_multiple_descriptions(self, preprocessed_reports: List[Dict[str, Any]], dimension: str, year: str, count: int) -> List[tuple]:
        """ä»æŠ¥å‘Šä¸­æå–å¤šä¸ªä¸åŒçš„æè¿°"""
        results = []
        exclude_texts = []
        
        for _ in range(count * 2):
            desc, master = self._extract_rich_description(preprocessed_reports, dimension, year, exclude_texts)
            if desc and desc not in exclude_texts:
                results.append((desc, master))
                exclude_texts.append(desc)
                if len(results) >= count:
                    break
        
        return results

    def _is_valid_llm_title(self, title: str, used_titles: List[str] = None) -> bool:
        """æ£€æŸ¥LLMè¿”å›çš„æ ‡é¢˜æ˜¯å¦æœ‰æ•ˆ
        
        æœ‰æ•ˆæ ‡é¢˜æ¡ä»¶ï¼š
        1. é•¿åº¦ä¸º2-5ä¸ªä¸­æ–‡å­—ç¬¦
        2. ä¸æ˜¯æŠ½è±¡è¡¨è¾¾ï¼ˆå¦‚"äº‹ä¸šå…±è¯†"ã€"è´¢å¯Œå˜åŒ–"ï¼‰
        3. ä¸æ˜¯æ–­è¯/ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        4. æœªè¢«ä½¿ç”¨è¿‡
        """
        if used_titles is None:
            used_titles = []
            
        if not title:
            return False
            
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€ç¬¦å·
        clean_title = title.replace("âœ¨", "").replace("âš¡", "").strip()
        
        # æ£€æŸ¥é•¿åº¦ï¼ˆ2-5ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', clean_title)
        if len(chinese_chars) < 2 or len(chinese_chars) > 5:
            return False
        
        # æ–­è¯æ£€æµ‹ - ä»¥ä¸‹ç»“å°¾çš„æ ‡é¢˜æ˜¯ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        broken_endings = [
            "å°†", "æŠŠ", "è¢«", "è®©", "ä½¿", "ç»™", "å‘", "å¾€", "æœ",  # ä»‹è¯/åŠ©è¯
            "çš„", "åœ°", "å¾—", "ç€", "äº†", "è¿‡",  # åŠ©è¯
            "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "åŠ",  # åŠ¨è¯/è¿è¯
            "è€Œ", "ä½†", "å´", "å¹¶", "ä¸”", "ä¹Ÿ", "éƒ½",  # è¿è¯/å‰¯è¯
            "èƒ½", "ä¼š", "å¯", "è¦", "åº”", "è¯¥", "éœ€",  # èƒ½æ„¿åŠ¨è¯
            "å¾ˆ", "å¤ª", "æœ€", "æ›´", "è¾ƒ", "æ¯”",  # ç¨‹åº¦å‰¯è¯
            "è¿™", "é‚£", "å…¶", "æŸ", "æ¯", "å„",  # æŒ‡ç¤ºè¯
            "è§†", "å½“", "ä¸º", "æˆ", "åš", "å¦‚", "è‹¥",  # åŠ¨è¯/è¿è¯
            "ä»", "è‡ª", "äº", "è‡³", "åˆ°", "ä»¥", "å› ",  # ä»‹è¯
            "å¯¹", "å…³", "ç»", "é€š", "æŒ‰", "æ®"  # ä»‹è¯
        ]
        if clean_title and clean_title[-1] in broken_endings:
            return False
        
        # æ–­è¯æ£€æµ‹ - ä»¥ä¸‹å¼€å¤´çš„æ ‡é¢˜æ˜¯ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        broken_beginnings = [
            "çš„", "åœ°", "å¾—", "äº†", "ç€", "è¿‡",  # åŠ©è¯
            "å’Œ", "ä¸", "æˆ–", "åŠ", "å¹¶", "ä¸”",  # è¿è¯
            "è€Œ", "ä½†", "å´", "åˆ™", "ä¾¿", "å³"  # è¿è¯
        ]
        if clean_title and clean_title[0] in broken_beginnings:
            return False
        
        # æ£€æµ‹å¸¸è§çš„æ–­è¯æ¨¡å¼ï¼ˆä¸å®Œæ•´çŸ­è¯­ï¼‰
        broken_patterns = [
            r'^[å°†æŠŠè¢«è®©ä½¿ç»™å‘å¾€æœ].+[è§†å½“ä¸ºæˆåš]$',  # å¦‚"å°†å¥èº«è§†"
            r'^.+[æ˜¯åœ¨æœ‰]$',  # å¦‚"æœºä¼šæ˜¯"ã€"å‘å±•åœ¨"
            r'^[åœ¨ä»äº].+$',  # å¦‚"åœ¨äº‹ä¸š"ï¼ˆé™¤éåé¢è¿˜æœ‰å†…å®¹ï¼‰
            r'^å…³äº.+$',  # å¦‚"å…³äºè´¢å¯Œ"
            r'^å¯¹äº.+$',  # å¦‚"å¯¹äºå¥åº·"
        ]
        for pattern in broken_patterns:
            if re.match(pattern, clean_title):
                return False
        
        # æŠ½è±¡/æ— æ•ˆæ ‡é¢˜é»‘åå•
        invalid_titles = [
            "äº‹ä¸šå…±è¯†", "è´¢å¯Œå…±è¯†", "æƒ…æ„Ÿå…±è¯†", "å¥åº·å…±è¯†",
            "äº‹ä¸šå˜åŒ–", "è´¢å¯Œå˜åŒ–", "æƒ…æ„Ÿå˜åŒ–", "å¥åº·å˜åŒ–",
            "äº‹ä¸šè¿åŠ¿", "è´¢å¯Œè¿åŠ¿", "æƒ…æ„Ÿè¿åŠ¿", "å¥åº·è¿åŠ¿",
            "è¿åŠ¿èµ°å‘", "å¥åº·çŠ¶å†µ", "è´¢å¯Œåˆ†æ", "äº‹ä¸šåˆ†æ",
            "å¹´åº¦è¿åŠ¿", "æ•´ä½“è¿åŠ¿", "ç»¼åˆè¿åŠ¿", "å…±è¯†è§‚ç‚¹",
            "ç‹¬ç‰¹è§‚ç‚¹", "å‘½ç†å˜æ•°", "æ ¸å¿ƒå…±è¯†", "å¹´åº¦åˆ†æ",
            "å¹´ä»½", "äº‹ä¸š", "è´¢å¯Œ", "æƒ…æ„Ÿ", "å¥åº·", "è¿åŠ¿",
            "å…±è¯†", "å˜åŒ–", "åˆ†æ", "è§‚ç‚¹"
        ]
        if clean_title in invalid_titles:
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²ä½¿ç”¨
        if title in used_titles or clean_title in used_titles:
            return False
            
        return True

    def _extract_node_title(self, description: str, dimension: str, node_type: str, used_titles: List[str] = None) -> str:
        """ä»æè¿°ä¸­æå–æˆ–ç”ŸæˆèŠ‚ç‚¹æ ‡é¢˜
        
        ä¼˜å…ˆä½¿ç”¨LLMè¿”å›çš„æ ‡é¢˜ï¼Œå¦‚æœæ— æ•ˆåˆ™ä»æè¿°ä¸­æå–å…³é”®è¯
        """
        if used_titles is None:
            used_titles = []
        
        # æ ¹æ®èŠ‚ç‚¹ç±»å‹æ·»åŠ å‰ç¼€
        type_prefix = {"consensus": "", "unique": "âœ¨", "variable": "âš¡"}
        prefix = type_prefix.get(node_type, "")
        
        # æ ¸å¿ƒå…³é”®è¯åº“ - ç”¨äºä»æè¿°ä¸­åŒ¹é…
        keyword_map = {
            "career": [
                ("æ™‹å‡", "æ™‹å‡æœºé‡"), ("å‡è¿", "å‡è¿æœºä¼š"), ("åˆ›ä¸š", "åˆ›ä¸šæ—¶æœº"), ("è½¬å‹", "è½¬å‹å¥‘æœº"),
                ("è´µäºº", "è´µäººç›¸åŠ©"), ("å°äºº", "å°äººé˜²èŒƒ"), ("åˆä½œ", "åˆä½œæœºä¼š"), ("ç«äº‰", "ç«äº‰åŠ å‰§"),
                ("çªç ´", "çªç ´æ–¹å‘"), ("ç¨³å®š", "äº‹ä¸šç¨³å®š"), ("å˜åŠ¨", "å·¥ä½œå˜åŠ¨"), ("å‹åŠ›", "å‹åŠ›ç®¡ç†"),
                ("å­¦ä¸š", "å­¦ä¸šè¿›æ­¥"), ("è€ƒè¯•", "è€ƒè¯•é¡ºåˆ©"), ("é¢è¯•", "é¢è¯•æœºä¼š"), ("é¡¹ç›®", "é¡¹ç›®æ¨è¿›"),
                ("å®¢æˆ·", "å®¢æˆ·æ‹“å±•"), ("å›¢é˜Ÿ", "å›¢é˜Ÿå»ºè®¾"), ("å†³ç­–", "é‡å¤§å†³ç­–"), ("æœºé‡", "æœºé‡é™ä¸´"),
                ("æŒ‘æˆ˜", "æŒ‘æˆ˜æ¥ä¸´"), ("è°ƒåŠ¨", "å²—ä½è°ƒåŠ¨"), ("ç¦»èŒ", "ç¦»èŒé£é™©"), ("é¢†å¯¼", "é¢†å¯¼èµè¯†"),
                ("åå£°", "åå£°æå‡"), ("èŒä½", "èŒä½å˜åŠ¨"), ("ä¸šç»©", "ä¸šç»©æå‡"), ("èƒ½åŠ›", "èƒ½åŠ›æå‡"),
                ("äººè„‰", "äººè„‰æ‹“å±•"), ("èµ„æº", "èµ„æºè·å–"), ("æˆé•¿", "ä¸ªäººæˆé•¿"), ("åˆ›æ–°", "åˆ›æ–°æœºä¼š")
            ],
            "wealth": [
                ("åè´¢", "åè´¢è¿æ—º"), ("æ­£è´¢", "æ­£è´¢ç¨³å¥"), ("ç ´è´¢", "ç ´è´¢é£é™©"), ("æŠ•èµ„", "æŠ•èµ„æœºä¼š"),
                ("ç†è´¢", "ç†è´¢è§„åˆ’"), ("æ”¶å…¥", "æ”¶å…¥å¢é•¿"), ("è´¢è¿", "è´¢è¿èµ°å‘"), ("å®ˆè´¢", "å®ˆè´¢ä¸ºä¸Š"),
                ("æ¨ªè´¢", "æ¨ªè´¢ä¿¡å·"), ("è€—è´¢", "è€—è´¢è­¦ç¤º"), ("è´¢åº“", "è´¢åº“å……å®"), ("èµ„äº§", "èµ„äº§é…ç½®"),
                ("å€ºåŠ¡", "å€ºåŠ¡é£é™©"), ("å¼€æº", "å¼€æºèŠ‚æµ"), ("èµŒåš", "å¿ŒèµŒåš"), ("å€Ÿè´·", "å€Ÿè´·è°¨æ…"),
                ("å‘è´¢", "å‘è´¢æ—¶æœº"), ("æ”¶ç›Š", "æ”¶ç›Šå›æŠ¥"), ("äºæŸ", "äºæŸé¢„è­¦"), ("æˆ¿äº§", "æˆ¿äº§æŠ•èµ„"),
                ("è‚¡ç¥¨", "è‚¡å¸‚æŠ•èµ„"), ("åŠ è–ª", "åŠ è–ªæœºä¼š"), ("å¥–é‡‘", "å¥–é‡‘æ”¶å…¥"), ("é’±è´¢", "é’±è´¢æµåŠ¨"),
                ("ç”Ÿæ„", "ç”Ÿæ„ç»è¥"), ("å‰¯ä¸š", "å‰¯ä¸šæ”¶å…¥"), ("æ¶ˆè´¹", "æ¶ˆè´¹æ”¯å‡º"), ("ç»“ç®—", "è´¦åŠ¡ç»“ç®—")
            ],
            "emotion": [
                ("æ¡ƒèŠ±", "æ¡ƒèŠ±æ—ºç››"), ("å©šå§»", "å©šå§»è¿åŠ¿"), ("æ‹çˆ±", "æ‹çˆ±æœºä¼š"), ("æ„Ÿæƒ…", "æ„Ÿæƒ…å˜åŒ–"),
                ("å®¶åº­", "å®¶åº­å’Œç¦"), ("çŸ›ç›¾", "æ„Ÿæƒ…çŸ›ç›¾"), ("åˆ†ç¦»", "åˆ†ç¦»é£é™©"), ("å¤åˆ", "å¤åˆæœºä¼š"),
                ("è¯±æƒ‘", "å¤–ç•Œè¯±æƒ‘"), ("å­å¥³", "å­å¥³ç¼˜åˆ†"), ("å­¤ç‹¬", "å­¤ç‹¬æ„Ÿå¼º"), ("æ²Ÿé€š", "æ²Ÿé€šæ”¹å–„"),
                ("ä¿¡ä»»", "ä¿¡ä»»å±æœº"), ("ç»“å©š", "ç»“å©šæ—¶æœº"), ("ç¦»å©š", "ç¦»å©šé£é™©"), ("ç¬¬ä¸‰è€…", "ç¬¬ä¸‰è€…æ’è¶³"),
                ("æš—æ˜§", "æš—æ˜§å…³ç³»"), ("è¡¨ç™½", "è¡¨ç™½æ—¶æœº"), ("çº¦ä¼š", "çº¦ä¼šæœºä¼š"), ("æ€€å­•", "æ€€å­•ç¼˜åˆ†"),
                ("ç”Ÿè‚²", "ç”Ÿè‚²è®¡åˆ’"), ("çˆ¶æ¯", "çˆ¶æ¯å…³ç³»"), ("æœ‹å‹", "å‹æƒ…è¿åŠ¿"), ("ç¼˜åˆ†", "å§»ç¼˜åˆ°æ¥"),
                ("å¨˜å®¶", "å¨˜å®¶å…³ç³»"), ("çº·äº‰", "å…³ç³»çº·äº‰"), ("å†·æ·¡", "æ„Ÿæƒ…å†·æ·¡"), ("å‡æ¸©", "æ„Ÿæƒ…å‡æ¸©")
            ],
            "health": [
                ("å¥åº·", "å¥åº·çŠ¶æ€"), ("ç–¾ç—…", "ç–¾ç—…é¢„è­¦"), ("è°ƒå…»", "èº«ä½“è°ƒå…»"), ("å¿ƒç†", "å¿ƒç†å¥åº·"),
                ("ä¼‘æ¯", "ä¼‘æ¯è°ƒæ•´"), ("è¿åŠ¨", "è¿åŠ¨å¥èº«"), ("é¥®é£Ÿ", "é¥®é£Ÿè°ƒç†"), ("ç²¾ç¥", "ç²¾ç¥çŠ¶æ€"),
                ("ç–²åŠ³", "è¿‡åº¦ç–²åŠ³"), ("æ„å¤–", "æ„å¤–é˜²èŒƒ"), ("å¹³å®‰", "å¹³å®‰é¡ºé‚"), ("å‹åŠ›", "å‹åŠ›ç®¡ç†"),
                ("å…ç–«", "å…ç–«åŠ›å¼º"), ("ä½“è´¨", "ä½“è´¨è°ƒç†"), ("åº·å¤", "åº·å¤æœŸåˆ°"), ("è‚ èƒƒ", "è‚ èƒƒä¿å¥"),
                ("å¤±çœ ", "å¤±çœ å›°æ‰°"), ("ç„¦è™‘", "ç„¦è™‘æƒ…ç»ª"), ("æ‰‹æœ¯", "æ‰‹æœ¯é£é™©"), ("ä½é™¢", "ä½é™¢å¯èƒ½"),
                ("è¡€å…‰", "è¡€å…‰ä¹‹ç¾"), ("è½¦ç¥¸", "è½¦ç¥¸é˜²èŒƒ"), ("è·Œä¼¤", "è·Œä¼¤é£é™©"), ("å¤´ç—›", "å¤´ç—›å›°æ‰°"),
                ("è…¹éƒ¨", "è…¹éƒ¨ä¸é€‚"), ("ç–¼ç—›", "ç–¼ç—›é—®é¢˜"), ("ä¼ æŸ“", "ä¼ æŸ“é˜²æŠ¤"), ("æ…¢æ€§", "æ…¢æ€§ç—…ç®¡ç†")
            ]
        }
        
        # ä»æè¿°ä¸­åŒ¹é…å…³é”®è¯
        dim_keywords = keyword_map.get(dimension, [])
        matched_titles = []
        for keyword, title in dim_keywords:
            if keyword in description:
                full_title = f"{prefix}{title}" if prefix else title
                # æ£€æŸ¥æ˜¯å¦å·²ä½¿ç”¨ä¸”æ˜¯æœ‰æ•ˆæ ‡é¢˜
                if full_title not in used_titles and self._is_valid_llm_title(full_title, used_titles):
                    return full_title
                matched_titles.append(full_title)
        
        # å¦‚æœæ‰€æœ‰åŒ¹é…çš„æ ‡é¢˜éƒ½å·²ä½¿ç”¨ï¼Œå°è¯•åŠ åºå·åŒºåˆ†
        if matched_titles:
            base_title = matched_titles[0].replace(prefix, "")
            for i in range(2, 10):
                new_title = f"{prefix}{base_title}{i}" if prefix else f"{base_title}{i}"
                if new_title not in used_titles and self._is_valid_llm_title(new_title, used_titles):
                    return new_title
        
        # å…œåº•ï¼šè¿”å›ä¸€ä¸ªé€šç”¨ä½†æœ‰æ•ˆçš„æ ‡é¢˜
        fallback_titles = {
            "career": {
                "consensus": "äº‹ä¸šç¨³å¥",
                "unique": "è´µäººæ˜¾ç°",
                "variable": "å˜åŠ¨é£é™©"
            },
            "wealth": {
                "consensus": "è´¢è¿å¹³ç¨³",
                "unique": "åè´¢æœºä¼š",
                "variable": "ç ´è´¢é˜²èŒƒ"
            },
            "emotion": {
                "consensus": "æ„Ÿæƒ…é¡ºé‚",
                "unique": "æ¡ƒèŠ±æœºé‡",
                "variable": "æ„Ÿæƒ…æ³¢æŠ˜"
            },
            "health": {
                "consensus": "èº«ä½“åº·å¥",
                "unique": "å…»ç”Ÿè°ƒç†",
                "variable": "å¥åº·é¢„è­¦"
            }
        }
        
        dim_fallbacks = fallback_titles.get(dimension, fallback_titles["career"])
        type_fallback = dim_fallbacks.get(node_type, "è¿åŠ¿åˆ†æ")
        
        full_title = f"{prefix}{type_fallback}" if prefix else type_fallback
        if full_title not in used_titles:
            return full_title
        
        # å¦‚æœå…¨éƒ¨ç”¨å®Œï¼ŒåŠ åºå·
        for i in range(2, 10):
            new_title = f"{prefix}{type_fallback}{i}" if prefix else f"{type_fallback}{i}"
            if new_title not in used_titles:
                return new_title
        
        return full_title

    def _sanitize_result(self, result: Dict[str, Any], future_years: int, preprocessed_reports: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¸…æ´—å’Œæ ¡éªŒæ•°æ®ï¼Œåªåšæ ¡éªŒä¸ç”Ÿæˆæ–°èŠ‚ç‚¹
        """
        logger.info("å¼€å§‹æ¸…æ´—å›¾è°±æ•°æ®...")
        logger.info(f"è¾“å…¥result keys: {list(result.keys())}")
        logger.info(f"preprocessed_reports é•¿åº¦: {len(preprocessed_reports)}")
        
        current_year = datetime.datetime.now().year
        graph_data = result.get("graph_data", {"nodes": [], "edges": []})
        nodes = graph_data.get("nodes", [])
        edges = graph_data.get("edges", [])
        
        # æœªæ¥Nå¹´åŒ…å«å½“å‰å¹´
        target_years = [f"{current_year + i}å¹´" for i in range(future_years)]
        required_dims = ["career", "wealth", "emotion", "health"]
        
        valid_nodes = []
        global_used_titles = []
        
        # éå†æ‰€æœ‰èŠ‚ç‚¹è¿›è¡Œæ ¡éªŒ
        for node in nodes:
            props = node.get("properties", {})
            
            # æ ¡éªŒå¿…è¦å­—æ®µ
            if not props.get("name"):
                logger.warning(f"èŠ‚ç‚¹ç¼ºå°‘nameå­—æ®µï¼Œè·³è¿‡: {node.get('id')}")
                continue
            
            if not props.get("description"):
                logger.warning(f"èŠ‚ç‚¹ç¼ºå°‘descriptionå­—æ®µï¼Œè·³è¿‡: {node.get('id')}")
                continue
            
            # æ ¡éªŒæè¿°é•¿åº¦
            desc_len = len(props.get("description", ""))
            if desc_len < 100:
                logger.warning(f"èŠ‚ç‚¹æè¿°å¤ªçŸ­({desc_len}å­—)ï¼Œå°è¯•è¡¥å……: {props.get('name')}")
                # å°è¯•ä»åŸå§‹æŠ¥å‘Šä¸­è¡¥å……å†…å®¹
                dimension = props.get("dimension", "career")
                year = props.get("time", "")
                node_type = props.get("type", "consensus")
                master_name = props.get("master_name", "")
                
                # æå–æ›´å¤šå†…å®¹
                extra_desc, _ = self._extract_rich_description(preprocessed_reports, dimension, year)
                if extra_desc:
                    if node_type == "consensus":
                        props["description"] = props["description"] + " " + extra_desc
                    else:
                        props["description"] = f"ã€{master_name}è§‚ç‚¹ã€‘{extra_desc}"
            
            # æ ¡éªŒæ ‡é¢˜
            llm_title = props.get("name", "")
            dimension = props.get("dimension", "career")
            node_type = props.get("type", "consensus")
            
            if not self._is_valid_llm_title(llm_title, global_used_titles):
                # ä»æè¿°ä¸­æå–æ–°æ ‡é¢˜
                new_title = self._extract_node_title(
                    props.get("description", ""), 
                    dimension, 
                    node_type, 
                    global_used_titles
                )
                logger.info(f"æ ‡é¢˜æ— æ•ˆï¼Œé‡æ–°æå–: '{llm_title}' -> '{new_title}'")
                props["name"] = new_title
            
            global_used_titles.append(props["name"])
            valid_nodes.append(node)
        
        # é‡å»ºè¾¹ï¼ˆåªä¿ç•™ä¸¤ç«¯èŠ‚ç‚¹éƒ½å­˜åœ¨çš„è¾¹ï¼‰
        valid_node_ids = {n["id"] for n in valid_nodes}
        valid_edges = []
        for edge in edges:
            if edge.get("source") in valid_node_ids and edge.get("target") in valid_node_ids:
                valid_edges.append(edge)
        
        graph_data["nodes"] = valid_nodes
        graph_data["edges"] = valid_edges
        result["graph_data"] = graph_data
        
        logger.info(f"æ¸…æ´—åçš„èŠ‚ç‚¹æ•°é‡: {len(valid_nodes)}")
        logger.info(f"æ¸…æ´—åçš„è¾¹æ•°é‡: {len(valid_edges)}")
        
        # ç¡®ä¿ consensus å’Œ conflicts å­—æ®µå­˜åœ¨
        if not result.get("consensus"):
            result["consensus"] = [
                {"text": n["properties"].get("name", ""), "impact": n["properties"].get("impact", 7)}
                for n in valid_nodes if n["properties"].get("type") == "consensus"
            ][:10]
        if not result.get("conflicts"):
            result["conflicts"] = [
                {"text": n["properties"].get("name", ""), "impact": n["properties"].get("impact", 6)}
                for n in valid_nodes if n["properties"].get("type") == "variable"
            ][:10]
        
        logger.info(f"æœ€ç»ˆresultåŒ…å« keys: {list(result.keys())}")
        logger.info("æ¸…æ´—å®Œæˆï¼Œè¿”å›result")
        
        return result
